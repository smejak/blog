---
title: Path to ACCI (A Thousand Brains part 2)
date: "2021-07-04 15:00:00"
---

I have just finished reading *A Thousand Brains* by Jeff Hawkins, so as promised, here is a continuation of my previous post on its contents. Once again, spoiler alert. 

Continuing from the theoretical framework described in the Thousand Brains Theory, Jeff goes on to explain why today’s artificial intelligence, more specifically machine learning and deep learning, isn’t truly intelligent. This is something I have thought about intensely in the past year or so when I got more into ML development and saw first-hand how deep neural networks learned. Essentially, neural networks are interconnected column vectors, with data passing linearly from one column to the next (at least in the case of very simple neural networks). Each node in the column vector acts as a linear function, something of the form 
*f(x)=ax+b*. 
As the neural network outputs some predictions, those predictions are compared with the true expected values during training to calculate the loss, which is then propagated through all the nodes within the network by something called backpropagation. This modifies the parameters of the nodes (aka weights) based on the gradient of the loss function and that particular node’s contribution to it (that is determined by partial differentiation). This process continues during training until a set of optimal weights has been found and the network consistently makes correct predictions on the training data (or the whole thing goes south and you have to start over). Then there’s also validation and testing, but that is not really relevant to this discussion. Ok, so we can think of neural networks as a series of numbers that change their values, or rather more accurately as a series of linear functions that change their parameters (because a neural network is basically a one giant function). 

Is that all there is to intelligence? If you think about it, the brain is just a bunch of neurons that change the strengths of their connections based on our everyday experiences, maybe with the addition that some connections are lost through the years and new connections are formed. It doesn’t seem to be that simple though. The brain, and specifically the neocortex, operates in a completely different environment compared to neural networks. Neural networks are, to use Jeff Hawkins’ words, static; once you’ve trained them, they don’t change. It’s just a function. Inputs and outputs. The brain isn’t like that. Once you start, you don’t stop until you die. Well, technically you do partly turn off each day during sleep, but when you’re fully awake, the inputs are constantly changing, and you can learn many different things, and you can understand what those things are. A neural network doesn’t understand what it does, and it usually does very little. All a neural network sees are numbers, it couldn’t describe the difference between a cat and the game of chess, unless it was trained on text data that describes that very difference. Regardless, a neural network doesn’t have true representation of knowledge, and Jeff argues that the only way (or at least the most viable way) a machine can ever be truly intelligent is when it works on the same principles as the neocortex does, meaning:
-	it learns continuously
-	it learns through movement
-	it has the ability to learn many models of the world
-	it uses reference frames to store knowledge.

The question that automatically follows is, how do we build such machines? (Note: learning via movement doesn’t necessarily mean we need a physical robot, a virtual environment like the internet and a virtual body that can navigate it should be sufficient.) In one of his talks, Jeff Hawkins proposes a roadmap to true machine intelligence, or as I like to call it, the ACCI (Artificial Content & Context Intelligence).

![](roadmap.svg)

Current research is mainly focused on stages 1 and 2, or at least it is at these stages that we are seeing some promising results. Creating active dendrites that allow continuous learning is the next step, the question is what continuous task do we choose to test this new architecture? [This paper](https://arxiv.org/pdf/2106.07490.pdf) describes the biological properties of dendrites and the incredible complexity that needs to be replicated in artificial systems. Looking past these challenges, such research may lead to significant reduction in power consumption by the ML models and might lead us one step closer to true ACCI. Exciting times ahead!
